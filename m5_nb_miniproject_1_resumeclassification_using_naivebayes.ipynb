{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpatra85/ColabTF_EDU/blob/master/m5_nb_miniproject_1_resumeclassification_using_naivebayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Certification Programme in AI and MLOps\n",
        "## A programme by IISc and TalentSprint\n",
        "### Mini-Project 1: Resume Classification"
      ],
      "metadata": {
        "id": "LJY-HzQzCiJx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNCzT95pcrIN"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMBrLtAtcrIO"
      },
      "source": [
        "At the end of the mini-project, you will be able to :\n",
        "\n",
        "* perform data preprocessing, EDA and feature extraction on the Resume dataset\n",
        "* perform multinomial Naive Bayes classification on the Resume dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6Xda9CM9el9"
      },
      "source": [
        "### Dataset description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeRfk4Sb9h0r"
      },
      "source": [
        "The data is in CSV format, with two features: Category, and Resume.\n",
        "\n",
        "**Category** -  Industry sector to which the resume belongs to, and\n",
        "\n",
        "**Resume** - The complete CV (text) of the candidate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7ZV5ryMP07f"
      },
      "source": [
        "##  Grading = 10 Points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Pped-3NPaDV"
      },
      "source": [
        "## Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuxYf07d62Oo"
      },
      "source": [
        "Companies often receive thousands of resumes for each job posting and employ dedicated screening officers to screen qualified candidates. Finding suitable candidates for an open role from a database of 1000s of resumes can be a tough task. Automated resume categorization can speeden the candidate selection process. Such automation can really ease the tedious process of fair screening and shortlisting the right candidates and aid quick decision making.\n",
        "\n",
        "To learn more about this, click [here](https://www.sciencedirect.com/science/article/pii/S187705092030750X)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t0FQjPs8o4s"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from pandas.plotting import scatter_matrix\n",
        "from sklearn import metrics\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from matplotlib.gridspec import GridSpec\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from wordcloud import WordCloud"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "standing-zimbabwe"
      },
      "source": [
        "#### Downloading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "universal-jonathan",
        "scrolled": true,
        "cellView": "form"
      },
      "source": [
        "#@title Download the dataset\n",
        "!wget -qq https://cdn.iisc.talentsprint.com/CDS/Datasets/UpdatedResumeDataSet.csv\n",
        "print(\"Data Downloaded Successfuly!!\")\n",
        "!ls | grep '.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3FkLI6wcaat"
      },
      "source": [
        "**Exercise 1: Read the UpdatedResumeDataset.csv dataset [0.5 Mark]**\n",
        "\n",
        "**Hint:** pd.read_csv()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIu-AOsB9GfD"
      },
      "source": [
        "# read the dataset\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvddL7X69NiB"
      },
      "source": [
        "### Pre-processing and EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA1d25HrzTGW"
      },
      "source": [
        "**Exercise 2: Display  all the categories of resumes and their counts in the dataset [0.5 Mark]**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C92ji6ZV9MWs"
      },
      "source": [
        "# Display the distinct categories of resume\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtIjY7ji9va5"
      },
      "source": [
        "# Display the distinct categories of resume and the number of records belonging to each category\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpHv50ojzvO5"
      },
      "source": [
        "**Exercise 3: Create the count plot of different categories [0.5 Mark]**\n",
        "\n",
        "**Hint:** Use `sns.countplot()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYwrK_5f93gP"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyXtz0Mr0NVB"
      },
      "source": [
        "**Exercise 4: Create a pie plot depicting the percentage of resume distributions category-wise [0.5 mark]**\n",
        "\n",
        "**Hint:** Use [plt.pie()](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.pie.html) and [plt.get_cmap](https://matplotlib.org/stable/tutorials/colors/colormaps.html) for color mapping the pie chart."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrpJDoGx-CuF"
      },
      "source": [
        "targetCounts = df['Category'].value_counts()\n",
        "targetLabels  = df['Category'].unique()\n",
        "# Make square figures and axes\n",
        "plt.figure(1, figsize=(25,25))\n",
        "the_grid = GridSpec(2, 2)\n",
        "\n",
        "# YOUR CODE HERE to display pie chart with color coding (eg. `coolwarm`)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJaznteJ1xr4"
      },
      "source": [
        "**Exercise 5: Convert all the `Resume` text to lower case [0.5 Mark]**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tf1wNRqb-Om2"
      },
      "source": [
        "# Convert all characters to lowercase\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cBXmdXpDIQJ"
      },
      "source": [
        "### Cleaning resumes' text data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvkRbRnM3ap7"
      },
      "source": [
        "**Exercise 6: Define a function to clean the resume text [2 Mark]**\n",
        "\n",
        "In the text there are special characters, urls, hashtags, mentions, etc. Remove the following:  \n",
        "\n",
        "* URLs: For reference click [here](https://stackoverflow.com/questions/11331982/how-to-remove-any-url-within-a-string-in-python)\n",
        "* RT | cc: For reference click [here](https://www.machinelearningplus.com/python/python-regex-tutorial-examples/)\n",
        "* Hashtags, # and Mentions, @\n",
        "* punctuations\n",
        "* extra whitespace\n",
        "\n",
        "PS: Use the provided reference similarly for removing any other such elements.\n",
        "\n",
        "After cleaning as above, store the Resume Text in a separate column (New Feature).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9Z-Oois_LWE"
      },
      "source": [
        "import re\n",
        "def cleanResume(resumeText):\n",
        "    # YOUR CODE HERE\n",
        "    #\n",
        "    #\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O2tR_IjDNWr"
      },
      "source": [
        " # apply the function defined above and save the\n",
        " # YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EnSuTubI_S1"
      },
      "source": [
        "sent_lens = []\n",
        "for i in df.cleaned_resume:\n",
        "    length = len(i.split())\n",
        "    sent_lens.append(length)\n",
        "\n",
        "print(len(sent_lens))\n",
        "print(max(sent_lens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktaCkU5yG5V0"
      },
      "source": [
        "### Stopwords removal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0XDeidJG-hc"
      },
      "source": [
        "The stopwords, for example, `and, the, was, and so forth` etc. appear very frequently in the text and are not helpful in the predictive process. Therefore these are usually removed for text analytics and text classification purposes.\n",
        "\n",
        "1. Tokenize the input words into individual tokens and store it in an array\n",
        "2. Using `nltk.corpus.stopwords`, remove the stopwords\n",
        "\n",
        "Hint: See Module 1 - Assignment 4 'Text Classification using Naive Bayes'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWP5cZL5_f0V"
      },
      "source": [
        "**Exercise 7: Use `nltk` package to find the most common words from the `cleaned resume` column [2 Marks]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRZSRsUT_G7h"
      },
      "source": [
        "**Hint:**\n",
        "* Use `nltk.FreqDist`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvSiTsqqXjeT"
      },
      "source": [
        "# stopwords\n",
        "# YOUR CODE HERE to print the stopwords in english language"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0v9bZeMwX0lN"
      },
      "source": [
        "# most common words\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYAJU8teYBlv"
      },
      "source": [
        "# YOUR CODE HERE to show the most common word using WordCloud"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpxqmsM6TmJu"
      },
      "source": [
        "**Exercise 8: Convert the categorical variable `Category` to a numerical feature and make a different column, which can be treated as the target variable [0.5 Mark]**\n",
        "\n",
        "**Hint:** Use [`sklearn.preprocessing.LabelEncoder()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kg4QNr7DYSJ5"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUauwAsOn31f"
      },
      "source": [
        "### Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTTiMQvYA77m"
      },
      "source": [
        "**Exercise 9: Convert the text to feature vectors by applying `tfidf vectorizer` to the Label encoded category made above [2 Marks]**\n",
        "\n",
        "`TF-IDF`will tokenize documents, learn the vocabulary, inverse document frequency weightings, and allow you to encode new documents\n",
        "\n",
        "**Hint:** Use [`TfidfVectorizer()`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUsZIUQyY_wX"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPTb3I-Oon2D"
      },
      "source": [
        "## Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocjoqFlCFJn3"
      },
      "source": [
        "**Exercise 10: Split the data into train and test sets. Apply Naive Bayes Classifier (MultinomialNB) and evaluate the model predictions [1 mark]**\n",
        "\n",
        "**Hint:** Use Vectorized features made above as X and Labelled category as y."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYoRwtlWobMI"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mr0xI6QwFN-Q"
      },
      "source": [
        "## Optional: Create a Gradio based web interface to test and display the model predictions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install gradio"
      ],
      "metadata": {
        "id": "KgwC2kUG5Of3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio"
      ],
      "metadata": {
        "id": "DJyrVukE5RBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "eh4nuFwx5SKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "349aUvoInpJv"
      },
      "source": [
        "**Report Analysis**\n",
        "- Which method(s), other than TF-IDF could be used for text to vector conversion?\n",
        "- Discuss about the `alpha`, `class_prior` and `fit_prior` parameters in sklearn `MultinomialNB`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnFxtl619jg_"
      },
      "source": [
        "Dataset Source Reference: [Resume dataset](https://www.kaggle.com/gauravduttakiit/resume-dataset/download)"
      ]
    }
  ]
}