{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpatra85/ColabTF_EDU/blob/master/Copy_of_M5_AST_03_Transformer_Encoder_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNgLag1Euy3H"
      },
      "source": [
        "# Advanced Certification Programme in AI and MLOps\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment 3: Transformer Encoders, Self Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tdtrlAhvIHY"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "* understand the big picture of transformers\n",
        "* understand and work with the Textvectorization layer\n",
        "* understand and work with the embedding layer\n",
        "* understand the consept of self attention\n",
        "* explore transformer encoder and positional embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Big Picture"
      ],
      "metadata": {
        "id": "Tv8TQrtfwaNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5%20AST%205%20Big%20Picture.png\" width=800px/>\n",
        "</center>"
      ],
      "metadata": {
        "id": "ni6YCBwE7d_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above is the entire architecture of transformer. A TextVectorization layer, Embedding layer, an Encoder and a Decoder.\n",
        "\n",
        "Transformer architecture follows an encoder-decoder structure. The encoder, on the left-hand side, is tasked with mapping an input sequence to a sequence of continuous representations; the decoder, on the right-hand side, receives the output of the encoder together with the decoder output at the previous time step to generate an output sequence."
      ],
      "metadata": {
        "id": "AxzUg64Jdv-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Transformer architecture was originally designed for translation. In the encoder, the attention layers can use all the words in a sentence (since, as we just saw, the translation of a given word can be dependent on what is after as well as before it in the sentence). The decoder, however, works sequentially and can only pay attention to the words in the sentence that it has already translated (so, only the words before the word currently being generated). For example, when we have predicted the first three words of the translated target, we give them to the decoder which then uses all the inputs of the encoder to try to predict the fourth word.\n",
        "\n",
        "To speed things up during training (when the model has access to target sentences), the decoder is fed the whole target, but it is not allowed to use future words (if it had access to the word at position 2 when trying to predict the word at position 2, the problem would not be very hard!). For instance, when trying to predict the fourth word, the attention layer will only have access to the words in positions 1 to 3."
      ],
      "metadata": {
        "id": "rgVPmcTjnORs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5_AST_05_Image1_Transformer.png\" width=900px/>\n",
        "</center>"
      ],
      "metadata": {
        "id": "VtUillZR7eEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment decoder will not form the topic of discussion, the main focus will be on the Transformer Encoder.\n",
        "This has been discussed in detail in the later sections of this notebook."
      ],
      "metadata": {
        "id": "geRywC9LeHjR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Description"
      ],
      "metadata": {
        "id": "ECysu18IoMaW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The IMDb Movie Reviews dataset is a binary sentiment analysis dataset consisting of 50,000 reviews from the Internet Movie Database (IMDb) labeled as positive or negative. The dataset contains an even number of positive and negative reviews."
      ],
      "metadata": {
        "id": "XLrl_aUaoPam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset is processed and used in the later sections of this notebook."
      ],
      "metadata": {
        "id": "KzZ2UaHyoS6z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YzfoPvJDiTX"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjoZJWGErxGf"
      },
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBPPuGmBlDIN",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M5_AST_03_Transformer_Encoder_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "\n",
        "    ipython.magic(\"sx curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\")\n",
        "    ipython.magic(\"sx tar -xvzf aclImdb_v1.tar.gz\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://aimlops-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RH8Ecq9sbYU"
      },
      "source": [
        "### Importing Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os, pathlib, shutil, random\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras.utils import text_dataset_from_directory"
      ],
      "metadata": {
        "id": "qnUhfGmx9cup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  **Part A** : Text Pre-processing and Embedding Before Transformer Block"
      ],
      "metadata": {
        "id": "4F75jcgTzupQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TextVectorization"
      ],
      "metadata": {
        "id": "KtDuXYqa7cbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing the text data:\n",
        "  * Text standardization\n",
        "  * Text splitting\n",
        "  * Vocabulary indexing\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "TIv-Yv3EnQ4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A flowchart depicting the procedure or sequence of steps followed by a TextVectorization layer. 'Standardization' is taking care of basic preprocessing of text data such as removing the punctuation and converting the text to lower case. 'Tokenization' is giving the list of words from the sentence. Later, these words are represented with indices and with the help of embedding to get the vector encoding of indices."
      ],
      "metadata": {
        "id": "o590tI2BldJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5_AST_05_Transformer_Encoder_Text_data_prep.png\" width=650px/>\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "y-m7SHDzoHHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All these steps are performed in a TextVectorization Layer.\n",
        "\n",
        "\n",
        "*   Keras provides a TextVectorization layer which can be dropped directly into\n",
        "      - a tf.data pipeline **or**\n",
        "      - a Keras model\n",
        "\n",
        "*  MOREOVER, TextVectorization also handles both approaches of representing groups of words:\n",
        "      - Words as a set or Bag-of-words\n",
        "      - Words as a sequence\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_RGm-7ySooG0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a dummy dataset and a test sentence\n"
      ],
      "metadata": {
        "id": "8QdHTVk5K_wR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "\n",
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "# dataset_t = [\"I write, rewrite, and still rewrite again\"]\n",
        "#Q: Is the word 'still' in the dataset (vocabulary)? Is it there in the test_sentence?\n",
        "#Q: How many words in test_sentence?"
      ],
      "metadata": {
        "id": "njuS3JqcS-RQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function depicting TextVectorization layer"
      ],
      "metadata": {
        "id": "M9sp6faVpYYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A function is defined here to demonstrate the working of a TextVectorization layer. This function also compares two text data by making use of encodings and decodings."
      ],
      "metadata": {
        "id": "pB0aHkyhr3D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function will be called multiple times in the sub-sequent code cells to demonstrate TextVectorization with different parameters such as a combinations of monograms, bigrams and different modes of output."
      ],
      "metadata": {
        "id": "mwf1NCofvA_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To see the workings of TextVectorization\n",
        "def demonstrate_TxVec(text_vectorization, dataset, test_sen, mode=None):\n",
        "  # arguments:\n",
        "  text_vectorization.adapt(dataset) # Computes a vocabulary of string terms from tokens in a dataset\n",
        "  vocabulary = text_vectorization.get_vocabulary()\n",
        "  print(f\"vocabulary = {vocabulary}\")\n",
        "  print(f\"len(vocabulary) = {len(vocabulary)}\")\n",
        "\n",
        "  # To see how the the text_vec layer transforms/vectorizes the raw text\n",
        "  encoded_sentence = text_vectorization(test_sen)\n",
        "  print(f\"encoded sentence = {encoded_sentence}\")\n",
        "  print(f\"len(encoded sentence) = {len(encoded_sentence)}\")\n",
        "  # print(f\"encoded dataset_t = {text_vectorization(dataset_t)}\")\n",
        "\n",
        "  # decode back for comparison with test_sentence\n",
        "  if mode==\"int\":\n",
        "    inverse_vocab = dict(enumerate(vocabulary)) # making a dictionary to decode embeddings\n",
        "    print(f\"inverse_vocab = {inverse_vocab}\")\n",
        "    decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
        "    print(f\"decoded sentence = {decoded_sentence}\")\n",
        "\n",
        "  if mode==\"multi_hot\":\n",
        "    for token in vocabulary:\n",
        "      print(f\"{token}:\\t {text_vectorization(token)}\")\n",
        "\n",
        "  if mode==\"count\":\n",
        "    for token in vocabulary:\n",
        "      print(f\"{token}:\\t {text_vectorization(token)}\")\n",
        "\n",
        "  if mode==\"tf\":\n",
        "    for token in vocabulary:\n",
        "      print(f\"{token}:\\t {text_vectorization(token)}\")\n",
        "\n",
        "  print(f\"test_sentence = {test_sen}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "eqL6sCaWTnTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instantiating a TextVectorization object with output mode as integer"
      ],
      "metadata": {
        "id": "IMKHL4rarg_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q: What 3 things does a TV layer do?\n",
        "# instantiating a TextVectorization layer/object\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",  # int is defualt. We will see different kinds of modes\n",
        "    # we can use custom fucntions for standardizing and splitting the text - see Chollet\n",
        "    # standardize=custom_standardization_fn,\n",
        "    # split=custom_split_fn,\n",
        ")\n",
        "\n",
        "demonstrate_TxVec(text_vectorization, dataset, test_sentence, mode=\"int\")\n",
        "\n",
        "# Q: What is a vocabulary?\n",
        "# Q: No. of tokens in vocabulary?\n",
        "# Q: Length of encoded_sentence (output of TV layer)?\n",
        "# Q: Type of elements in encoded_sentence (embedding)?\n",
        "# Q: Is decoded sentence the same as the test_sentence? Why?\n",
        "\n",
        "# dataset = [\n",
        "#     \"I write, erase, rewrite\",\n",
        "#     \"Erase again, and then\",\n",
        "#     \"A poppy blooms.\",\n",
        "# ]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0AS4G6RqMEU",
        "outputId": "3863a84c-3af6-4f5d-9993-f4d89e777df5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary = ['', '[UNK]', 'erase', 'write', 'then', 'rewrite', 'poppy', 'i', 'blooms', 'and', 'again', 'a']\n",
            "len(vocabulary) = 12\n",
            "encoded sentence = [ 7  3  5  9  1  5 10]\n",
            "len(encoded sentence) = 7\n",
            "inverse_vocab = {0: '', 1: '[UNK]', 2: 'erase', 3: 'write', 4: 'then', 5: 'rewrite', 6: 'poppy', 7: 'i', 8: 'blooms', 9: 'and', 10: 'again', 11: 'a'}\n",
            "decoded sentence = i write rewrite and [UNK] rewrite again\n",
            "test_sentence = I write, rewrite, and still rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instantiating a TextVectorization object with output mode as integer and considering bigrams"
      ],
      "metadata": {
        "id": "yd_qVVlIvr5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bigrams with integer encoding\n",
        "text_vectorization = TextVectorization(\n",
        "    ngrams = 2,\n",
        "    output_mode=\"int\",\n",
        "    # output_sequence_length=20\n",
        ")\n",
        "demonstrate_TxVec(text_vectorization, dataset, test_sentence, mode=\"int\")\n",
        "# Q: Can we have 'rewrite erase' in vocab?\n",
        "# Q: Why the extra 'i write'?\n",
        "# Q: Why the extra 'UNK' ?\n",
        "\n",
        "# dataset = [\n",
        "#     \"I write, erase, rewrite\",\n",
        "#     \"Erase again, and then\",\n",
        "#     \"A poppy blooms.\",\n",
        "# ]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jepK4bjqWq1D",
        "outputId": "5695530c-daf3-447a-d2bd-ddf297f51cdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary = ['', '[UNK]', 'erase', 'write erase', 'write', 'then', 'rewrite', 'poppy blooms', 'poppy', 'i write', 'i', 'erase rewrite', 'erase again', 'blooms', 'and then', 'and', 'again and', 'again', 'a poppy', 'a']\n",
            "len(vocabulary) = 20\n",
            "encoded sentence = [10  4  6 15  1  6 17  9  1  1  1  1  1]\n",
            "len(encoded sentence) = 13\n",
            "inverse_vocab = {0: '', 1: '[UNK]', 2: 'erase', 3: 'write erase', 4: 'write', 5: 'then', 6: 'rewrite', 7: 'poppy blooms', 8: 'poppy', 9: 'i write', 10: 'i', 11: 'erase rewrite', 12: 'erase again', 13: 'blooms', 14: 'and then', 15: 'and', 16: 'again and', 17: 'again', 18: 'a poppy', 19: 'a'}\n",
            "decoded sentence = i write rewrite and [UNK] rewrite again i write [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "test_sentence = I write, rewrite, and still rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instantiating a TextVectorization object with output mode as integer, maximum tokens as 20 and output mode as `'multi_hot'` encodings"
      ],
      "metadata": {
        "id": "dEz-sSvJvyoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Unigrams with binary encoding\n",
        "text_vectorization = TextVectorization(\n",
        "    ngrams = 1, # Default value\n",
        "    max_tokens = 20, # let's change this value to 8 and see\n",
        "    output_mode=\"multi_hot\",\n",
        ")\n",
        "demonstrate_TxVec(text_vectorization, dataset, test_sentence, mode=\"multi_hot\")\n",
        "\n",
        "# Recall we saw this in tutorial 2\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UXYhaDYF9xg",
        "outputId": "0418871b-c623-445b-f792-874ecdf8b88f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary = ['[UNK]', 'erase', 'write', 'then', 'rewrite', 'poppy', 'i', 'blooms', 'and', 'again', 'a']\n",
            "len(vocabulary) = 11\n",
            "encoded sentence = [1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0.]\n",
            "len(encoded sentence) = 11\n",
            "[UNK]:\t [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "erase:\t [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "write:\t [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "then:\t [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "rewrite:\t [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "poppy:\t [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "i:\t [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "blooms:\t [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "and:\t [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "again:\t [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "a:\t [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "test_sentence = I write, rewrite, and still rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instantiating a TextVectorization object with output mode as `'multi_hot'` and considering bigrams"
      ],
      "metadata": {
        "id": "-fR0OClawQIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams = 2, # bag of 2 or less words. See cell o/p\n",
        "    output_mode=\"multi_hot\",\n",
        ")\n",
        "# Bi-grams typically perform better than unigrams- Word order matters!\n",
        "demonstrate_TxVec(text_vectorization, dataset, test_sentence, mode=\"multi_hot\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJnKVKRhGk-E",
        "outputId": "1a92f565-bb11-4131-85f7-3fea97428103"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary = ['[UNK]', 'erase', 'write erase', 'write', 'then', 'rewrite', 'poppy blooms', 'poppy', 'i write', 'i', 'erase rewrite', 'erase again', 'blooms', 'and then', 'and', 'again and', 'again', 'a poppy', 'a']\n",
            "len(vocabulary) = 19\n",
            "encoded sentence = [1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0.]\n",
            "len(encoded sentence) = 19\n",
            "[UNK]:\t [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "erase:\t [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "write erase:\t [0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "write:\t [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "then:\t [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "rewrite:\t [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "poppy blooms:\t [0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "poppy:\t [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "i write:\t [0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "i:\t [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "erase rewrite:\t [0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "erase again:\t [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
            "blooms:\t [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "and then:\t [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
            "and:\t [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "again and:\t [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0.]\n",
            "again:\t [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "a poppy:\t [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
            "a:\t [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "test_sentence = I write, rewrite, and still rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instantiating a TextVectorization object with output mode as `'count'` and considering monograms"
      ],
      "metadata": {
        "id": "_X9DrVGTweV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bigrams with token counts\n",
        "text_vectorization = TextVectorization(\n",
        "    ngrams = 1,\n",
        "    # max_tokens = 8,\n",
        "    output_mode=\"count\",\n",
        ")\n",
        "demonstrate_TxVec(text_vectorization, dataset, test_sentence, mode=\"count\")\n",
        "#Q: which token comes twice in the test_sentence?\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_itfCn0aGreY",
        "outputId": "83419ae4-97cf-4041-9c2d-cff850a8c498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x787540b327a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary = ['[UNK]', 'erase', 'write', 'then', 'rewrite', 'poppy', 'i', 'blooms', 'and', 'again', 'a']\n",
            "len(vocabulary) = 11\n",
            "encoded sentence = [1. 0. 1. 0. 2. 0. 1. 0. 1. 1. 0.]\n",
            "len(encoded sentence) = 11\n",
            "[UNK]:\t [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "erase:\t [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "write:\t [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "then:\t [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "rewrite:\t [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "poppy:\t [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "i:\t [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "blooms:\t [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "and:\t [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "again:\t [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "a:\t [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "test_sentence = I write, rewrite, and still rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instantiating a TextVectorization object with output mode as `'tf_idf'` and considering monograms"
      ],
      "metadata": {
        "id": "ywFrwXxLwlD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bigrams with TF-IDF weighted outputs\n",
        "text_vectorization = TextVectorization(\n",
        "    ngrams = 1,\n",
        "    output_mode=\"tf_idf\",\n",
        ")\n",
        "demonstrate_TxVec(text_vectorization, dataset, test_sentence, mode=\"tf\")\n",
        "\n",
        "# Often leads to 1% increase in perfermonance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3rAdK92Gyfq",
        "outputId": "0b21eda6-c906-4363-bc17-2addab9a75c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x787540b339a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary = ['[UNK]', 'erase', 'write', 'then', 'rewrite', 'poppy', 'i', 'blooms', 'and', 'again', 'a']\n",
            "len(vocabulary) = 11\n",
            "encoded sentence = [0.8939764  0.         0.91629076 0.         1.8325815  0.\n",
            " 0.91629076 0.         0.91629076 0.91629076 0.        ]\n",
            "len(encoded sentence) = 11\n",
            "[UNK]:\t [0.8939764 0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.       ]\n",
            "erase:\t [0.        0.6931472 0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.       ]\n",
            "write:\t [0.         0.         0.91629076 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.        ]\n",
            "then:\t [0.         0.         0.         0.91629076 0.         0.\n",
            " 0.         0.         0.         0.         0.        ]\n",
            "rewrite:\t [0.         0.         0.         0.         0.91629076 0.\n",
            " 0.         0.         0.         0.         0.        ]\n",
            "poppy:\t [0.         0.         0.         0.         0.         0.91629076\n",
            " 0.         0.         0.         0.         0.        ]\n",
            "i:\t [0.         0.         0.         0.         0.         0.\n",
            " 0.91629076 0.         0.         0.         0.        ]\n",
            "blooms:\t [0.         0.         0.         0.         0.         0.\n",
            " 0.         0.91629076 0.         0.         0.        ]\n",
            "and:\t [0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.91629076 0.         0.        ]\n",
            "again:\t [0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.91629076 0.        ]\n",
            "a:\t [0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.91629076]\n",
            "test_sentence = I write, rewrite, and still rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TextVectorization in Keras Model\n",
        "\n",
        "This technique is useful for production: For a stand-alone model.\n",
        "\n",
        "Load a saved model and add a 'text_vectorization' layer to it"
      ],
      "metadata": {
        "id": "yCQHEwZkFXmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a model architecture in a function"
      ],
      "metadata": {
        "id": "6QOfDPiNjTfJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The processed inputs from will be later fed to this model."
      ],
      "metadata": {
        "id": "lyQ2i5GIWxDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dense network which may be used repetitively\n",
        "def get_model(max_tokens=20000, hidden_dim=16):\n",
        "  inputs = keras.Input(shape=(max_tokens,))\n",
        "  x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "  outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "  model = keras.Model(inputs, outputs)\n",
        "  model.compile(optimizer=\"rmsprop\",\n",
        "                    loss=\"binary_crossentropy\",\n",
        "                    metrics=[\"accuracy\"])\n",
        "  return model"
      ],
      "metadata": {
        "id": "TdyrQSXQjZEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  model=get_model()\n",
        "  inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "  processed_inputs = text_vectorization(inputs)\n",
        "  outputs = model(processed_inputs) #some trained model\n",
        "  inference_model = keras.Model(inputs, outputs)\n",
        "  inference_model.summary()\n",
        "except:\n",
        "  pass"
      ],
      "metadata": {
        "id": "ImGc3tyXLgfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A discussion related to the method depicted above will be demonstrated in the next assignment.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ILbGkb8LFYCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation Example"
      ],
      "metadata": {
        "id": "UkoDTKmd900K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pre-processed version of the IMDB dataset provided by Keras was used in the previous assignments."
      ],
      "metadata": {
        "id": "3XIAoz7enlj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Originally IMDB dataset contains the *train* and the *test* folders.\n",
        "Here, the original dataset will be used and pre-processing related to it will be explored."
      ],
      "metadata": {
        "id": "xcKJhCGE1yTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List subdirectories\n",
        "!cd aclImdb && ls -d */"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46rYfHyZrqbs",
        "outputId": "6d8c372a-0b70-4e26-e2b7-ced9cf5a7b2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test/  train/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove unnecessary folder\n",
        "!rm -r aclImdb/train/unsup"
      ],
      "metadata": {
        "id": "mFgcefQNLh-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualise a sample\n",
        "!cat aclImdb/train/pos/4077_10.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DWQXmaYLuha",
        "outputId": "c1bbaab2-c37d-4137-cbf1-4e5a6174675f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a validation directory and move 20% of the train data to it"
      ],
      "metadata": {
        "id": "Za7BufC_2q_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# move 20% of the training data to the validation folder\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    # random.Random(1337).shuffle(files) # We should shuffle. Only commenting for demonstration\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)"
      ],
      "metadata": {
        "id": "9BseO3_sLukB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create batches of data using `text_dataset_from_directory`"
      ],
      "metadata": {
        "id": "YBeeGi5S8pbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset using utility\n",
        "batch_size = 32\n",
        "\n",
        "# Q: Name other such utilities seen earlier ?\n",
        "train_ds = text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size)\n",
        "\n",
        "val_ds = text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size)\n",
        "\n",
        "test_ds = text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size)\n",
        "\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x) #replace x,y with x. That is remove labels, just keep text data.\n"
      ],
      "metadata": {
        "id": "cDLoEh2CLwB6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17103c78-b1c1-487f-9a51-21eef6c5cf9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 20000, 5000, and 25000 records in train, validation, and test directories with two class as positive and negative."
      ],
      "metadata": {
        "id": "htOQeKY98_3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check shapes\n",
        "for inputs, targets in train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "\n",
        "    print(\"inputs[2]:\", inputs[2])\n",
        "    print(\"targets[2]:\", targets[2])\n",
        "    break"
      ],
      "metadata": {
        "id": "u_03-Oj9LwD5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "543c1dd9-eecd-4939-ee6a-2b369f9689a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[2]: tf.Tensor(b'Problem with these type of movies is that literally dozens of them are being made each year. Luckily for use only a handful are given a theatrical release, while the others are being pushed straight to video or TV, such as this movie.<br /><br />The foremost problem of this movie is really its originality. It\\'s one of those movies which uses the \"Die Hard\" formula of a tough but troubled guy being at the wrong place at the wrong time. In this case it\\'s a character played by Casper Van Dien, who works for a security agency that thoroughly test safety procedures for companies and individuals. In this case he\\'s being send to a cruise ship, which of course gets hijacked. You can see this movie as a sort of mix of \"Die Hard\" and \"Air Force One\" and the movie doesn\\'t even try to conceal that those two movies were probably its biggest source of \\'inspiration\\'. So really, you can\\'t regard this movie as an original one at all. It uses all of the clich\\xc3\\xa9s out of the book and this movie really doesn\\'t offer any surprises or anything that remotely resembles anything original.<br /><br />Like you can expect from a movie such as this, it has a very weak script. Or rather said, it features some very lazy writing. Like I said before, the movie features nothing original but also the actual story itself features some elements which are far from likely and are just plain ridicules truthfully. I mean, hijacking an huge cruise ship with only about 8 guys, of which halve only carry some small guns and then ask for a ransom of \\'only\\' 10 million dollars, for a ship that is about worth 10 times that amount is itself already quite ridicules. How do they even intend to split that money afterward? Every person gets just over a million or something? That\\'s hardly profitable for such a big and risky undertaking. And then there is the case of taking the passengers hostage. Somehow they manage to take all passengers on the huge ship hostage and they manage to put them inside one room, with only one guy with his pistol, which he can\\'t even seem to be able to hold right, watching them. You never see more than like 30 hostages however, as if they were all the people who were aboard at the time. Also when the Van Dien character goes looking for his son and vice versa, no matter which room they walk in through on the huge cruise ship, they always bump into each other instantly. Just some examples of the lazy writing within the movie.<br /><br />But it of course is an action flick, so the story of course becomes secondary. But then again, it\\'s not as if this movie features any good action at all. Halve of the actors look as if they had never hold a weapon before and the movie is filled with some ridicules slow-motion. It really becomes laughable at points.<br /><br />Of course the movie also doesn\\'t feature the best actors, though I must say that Casper Van Dien really isn\\'t a bad \\'action hero\\' and actor, as far as the genre and B-movie circuit is concerned. He just however also suffers from the same problem as Tom Cruise; no matter how old he is, he just never looks convincing enough to play the father of a teenager. Van Dien once started out as a promising new young actor but starring in movies like this really doesn\\'t help his career much. He\\'s probably capable of something better, though he is never really given the right opportunity to show it. All of the other actors also do a fair enough job but their characters are just so formulaic that they never truly become interesting.<br /><br />Oh well, it\\'s not the worst genre movie I have ever seen but it also ain\\'t exactly the most original or memorable one either.<br /><br />4/10', shape=(), dtype=string)\n",
            "targets[2]: tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing the data using TextVectorization layer of keras"
      ],
      "metadata": {
        "id": "sR63E5Ea9fDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing the data\n",
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,    # Q: What is the vocabular size?\n",
        "    output_mode=\"int\",        # Q: What will be the type of output for a token (say), 'amazing' ?\n",
        "    output_sequence_length=max_length,      # Q: What is the maximum length of review? Is it a fair assumption?\n",
        "    )\n",
        "\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n"
      ],
      "metadata": {
        "id": "Bqi5Z24gMK9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5_AST_05_Transformer_Encoder_Text_data_prep.png\" width=650px/>\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "lCFPfVAts1IW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize and compare the raw and processed data"
      ],
      "metadata": {
        "id": "J7sj-tcc9v2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's visualize the raw text and the vectorized (to int) text\n",
        "for text, label in train_ds:\n",
        "  print(text[0])\n",
        "  print(label[0])\n",
        "  break\n",
        "\n",
        "for int_of_text, label in int_train_ds:\n",
        "  print(int_of_text[0])\n",
        "  print(label[0])\n",
        "  break\n",
        "\n",
        "# Q: How can you verify whether the index of movie is 18?\n"
      ],
      "metadata": {
        "id": "nMUYGUqBx3K1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb04a7c5-4074-4493-e10d-8bedead1a619"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b\"This romantic adventure must have seemed shockingly subversive in its day. A wealthy upper class English woman schemes, plots and manipulates everyone around her for her own satisfaction. She uses her privileged position to embark on secret activities of a decidedly anti-social kind. There's a clever sex-role reversal as her activities prove her more daring and dashing than most of the male characters. But naturally there's a tall, dark and handsome stranger to keep up the love interest, and this wicked lady is not backward in coming forward when she meets the right man.<br /><br />The wishy-washy weakness and gullibility of every other character make the plot unconvincing in the extreme, but those who thirst for Romance will overlook that.\", shape=(), dtype=string)\n",
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "tf.Tensor(\n",
            "[   11    18     7   271   310    16     2  1074   898     3   732   113\n",
            "   583    19    48    79     7    34   222  2732  6508     5 10059   370\n",
            "     3  3003   193   815   299     8     4   669   212     5     4 10386\n",
            "  1548   325    24   193  3974     6    27 12800     6     1   551     8\n",
            "  2757    11   155     2  2424  9502   196    34  3215   905  4233     3\n",
            "     4   183   250    54  3618    15     4  3770   124     2  2058 19620\n",
            "    13     2    60   426    10    67    17    11    18    14     2  6625\n",
            "     5     2   796     6 11061    58    17  1475  5540  1733    33     2\n",
            "   153  6107    13     4  2357  5292     6  6389  1432   215    93     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0], shape=(600,), dtype=int64)\n",
            "tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector representation of the word movie"
      ],
      "metadata": {
        "id": "P01-UwZIAuzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization(\"movie\")\n",
        "# Q: What is the shape of the TV output?\n",
        "# Q: Why so many 0s?\n"
      ],
      "metadata": {
        "id": "Jl8iSY55z1A0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd3ace80-5a56-4254-87d9-4f3887ce80c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(600,), dtype=int64, numpy=\n",
              "array([18,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0])>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector representation of \"great movie\" and \"a fine story\""
      ],
      "metadata": {
        "id": "6oAL6gzUAzvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization([\"great movie\",\"a fine story\"])\n",
        "#Q: shape?"
      ],
      "metadata": {
        "id": "oCDXUf8bDRIz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1faa1311-c9d4-45df-d541-41931afc840f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 600), dtype=int64, numpy=\n",
              "array([[ 87,  18,   0, ...,   0,   0,   0],\n",
              "       [  4, 473,  64, ...,   0,   0,   0]])>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings"
      ],
      "metadata": {
        "id": "BL3w3DRqVy5B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Word embeddings are vector representations of words that achieve exactly this: they map human language into a structured geometric space.\n",
        "\n",
        "* dense (floats)\n",
        "* low-dimensional (1024 dims for large vocabs)"
      ],
      "metadata": {
        "id": "ex3lQf00YyM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two ways to obtain word embeddings:\n",
        "\n",
        "* Learn word embeddings jointly with the main task you care about (such as document classification or sentiment prediction). In this setup, you start with random word vectors and then learn word vectors, in the same way you learn the weights of a neural network. **Move away from manual feature engineering.**\n",
        "* Load into your model word embeddings that were precomputed using a different machine learning task than the one youre trying to solve. These are called pretrained word embeddings.\n",
        "\n",
        "**Q: Do two ways remind you of something we studied in CNNs ?**\n",
        "\n",
        "In this assignment the main agenda is to explore the Learning of word embeddings.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fMKg5tOpcgXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding Layer\n"
      ],
      "metadata": {
        "id": "t5OVct9NckRD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The procedure if as follows:\n",
        "\n",
        "*   Like a dictionary that **maps integer indices** (which stand for specific words) **to dense vectors**\n",
        "\n",
        "*   Input: a rank-2 tensor of integers, of shape (batch_size, sequence_length)\n",
        "*   Output: 3D floating-point tensor of shape (batch_size, sequence_length, embedding_dimensionality)\n",
        "*   WORD INDEX  EMBEDDING LAYER  CORRESPONDING WORD VEC\n",
        "\n",
        "*   Initial weights are random\n",
        "*   Learns specialized structure upon training\n",
        "\n"
      ],
      "metadata": {
        "id": "gdLRIpXwkQAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5%20AST5%20Embedding%20Layer.png\" width=750px/>\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "G91_UEMy-gpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define an LSTM architecture with an Embedding layer, and a Bidriectional layer"
      ],
      "metadata": {
        "id": "K_tTVNb4GhCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_tokens = 20000\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "# The Embedding layer\n",
        "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)  # the largest integer (i.e. word index) in the input\n",
        "                                                                             # should be no larger than 19999 (vocabulary size).\n",
        "# Q: What is the input to the Embedding layer?\n",
        "# Q: What is the dimension of the output embeddings\n",
        "# Q: In embedding layer shape, what are None and None ?\n",
        "\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "#Q: Weights in the embedding layer?\n",
        "#Hint: Dict; 1 input word => embedding of size ___ ."
      ],
      "metadata": {
        "id": "njaeQikslGr4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2cbd323-37db-4d35-bb12-124a6b7193d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 256)         5120000   \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 64)                73984     \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5194049 (19.81 MB)\n",
            "Trainable params: 5194049 (19.81 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model and make a prediction"
      ],
      "metadata": {
        "id": "0wwNyu9PG4Ts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model - This code cell can be commented for brevity\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10,callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "id": "fPbyk2A483fF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9da8ceb3-812b-4b31-a7de-db0167845576"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 63s 91ms/step - loss: 0.5242 - accuracy: 0.7364 - val_loss: 0.4515 - val_accuracy: 0.8146\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 43s 68ms/step - loss: 0.3548 - accuracy: 0.8620 - val_loss: 0.3123 - val_accuracy: 0.8754\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 40s 64ms/step - loss: 0.2880 - accuracy: 0.8972 - val_loss: 0.3978 - val_accuracy: 0.8666\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 33s 54ms/step - loss: 0.2443 - accuracy: 0.9168 - val_loss: 0.3372 - val_accuracy: 0.8798\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 31s 50ms/step - loss: 0.2075 - accuracy: 0.9320 - val_loss: 0.3356 - val_accuracy: 0.8784\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.1744 - accuracy: 0.9412 - val_loss: 0.3749 - val_accuracy: 0.8750\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.1582 - accuracy: 0.9488 - val_loss: 0.7414 - val_accuracy: 0.7850\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 30s 48ms/step - loss: 0.1423 - accuracy: 0.9574 - val_loss: 0.3714 - val_accuracy: 0.8806\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.1141 - accuracy: 0.9650 - val_loss: 0.4696 - val_accuracy: 0.8766\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 0.0985 - accuracy: 0.9717 - val_loss: 0.5165 - val_accuracy: 0.8732\n",
            "782/782 [==============================] - 18s 21ms/step - loss: 0.3375 - accuracy: 0.8615\n",
            "Test acc: 0.862\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # **Part B** : Building Encoder Transformer"
      ],
      "metadata": {
        "id": "L4lbyeZVxojj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Figure below shows the **Transformer Model Architecture** as per the paper [\"**Attention Is All You Need**\"](https://arxiv.org/pdf/1706.03762v6.pdf) .Here, we are going to implement **Encoder** and try to understand how it function. We are writing **'as per the paper'** to  mention this paper throughout this notebook. To completely understand the Encoder Transformer, it is imperative to understand Self Attention which is used inside Multi-head Attention. The data after passing the TextVectorization layer and Embedding layer will pass the Self Attention layer.\n",
        "<br><br>\n",
        "<center>\n",
        "<img src= \"https://cdn.extras.talentsprint.com/aiml/Experiment_related_data/Images/Transformer.png\" width=750px/>\n",
        "\n",
        "</center>"
      ],
      "metadata": {
        "id": "xEAlgHlEi360"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self Attention\n",
        "The attention mechanism being depicted in the picture below can be understood as the attention scores highlighting the most important features of the cat so that it can be identified."
      ],
      "metadata": {
        "id": "OoxkpGwjLs8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center>\n",
        "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/Attention%20scores%20pic.png\" width=700px/>\n",
        "</center>\n",
        "\n"
      ],
      "metadata": {
        "id": "hXf1hMTsaybL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the popular language models does not have just the term 'BERT' in their name but an important techique called 'self-attention'. Transformer-based architectures, which are primarily used in modelling language understanding tasks, eschew recurrence in neural networks and instead trust entirely on self-attention mechanisms to draw global dependencies between inputs and outputs."
      ],
      "metadata": {
        "id": "9WM_eM6KLGSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5%20AST5%20Self%20Attention%20Scores.png\" width=900px/>\n",
        "</center>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M_nEW1yj8Gzn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "outputs = sum(inputs * pairwise_scores(inputs, inputs))\n"
      ],
      "metadata": {
        "id": "3wTkPQOSJ7VJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to the self attention scores which are depicted in the picture, the word 'train pays' more attention to station rather than other words in consideration such as 'on' or the."
      ],
      "metadata": {
        "id": "tC2wZV8FNpuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The attention mechanism allows output to focus attention on input while producing output while the self-attention model allows inputs to interact with each other (i.e calculate attention of all other inputs wrt one input)."
      ],
      "metadata": {
        "id": "qOp6pGTsNnP2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Inside each attention head is a **Scaled Dot Product Self-Attention** operation, the operation returns a Attention vector as given by equation below:\n",
        "\n",
        "$$ Self Attention = softmax(\\frac{x^{T}_i x_j}{\\sqrt{d_k}})x_j $$\n",
        "\n",
        "The term  **$x^{T}_i x_j$** is dot product of input vector with itself. The  'pivot_vector' and the 'vector' forms the 'xi' and 'xj' of the above Self Attention function."
      ],
      "metadata": {
        "id": "gFRVERe-bRFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Demonstrating self_attention with dummy data"
      ],
      "metadata": {
        "id": "y5OQMPqTOGRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define a custom self attention function"
      ],
      "metadata": {
        "id": "weO9uy0NYcGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom self attention function\n",
        "def self_attention(input_sequence):\n",
        "  output = np.zeros(shape=input_sequence.shape)\n",
        "  for i, pivot_vector in enumerate(input_sequence): # iterate over each token in ip seq\n",
        "    scores = np.zeros(shape=(len(input_sequence), ))\n",
        "\n",
        "    for j, vector in enumerate(input_sequence):\n",
        "      scores[j] = np.dot(pivot_vector, vector.T)    # Pairwise scores\n",
        "\n",
        "    scores /= np.sqrt(input_sequence.shape[1]) # scale #[1] is the embedding dim\n",
        "    scores = tf.nn.softmax(scores)              # softmax\n",
        "    new_pivot_representation = np.zeros(shape=pivot_vector.shape)\n",
        "    for j, vector in enumerate(input_sequence):\n",
        "      new_pivot_representation += vector*scores[j] # weigthed sum\n",
        "    output[i] = new_pivot_representation\n",
        "  return output\n",
        "\n",
        "# Optional HW: Add to the code to print the attention_score matrix"
      ],
      "metadata": {
        "id": "tN4mtFwH7bWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Use a dummy data and find its vectors"
      ],
      "metadata": {
        "id": "qf8cvT-qYoDm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data first passes through the TextVectorization layer then through the Embedding layer and later the Self Attention scores are calculated"
      ],
      "metadata": {
        "id": "__9n0HmEZBoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, vectorize raw text using _________\n",
        "dummy_vocab = [\"movie was very nice\", \"film was good\"]\n",
        "text_vec = layers.TextVectorization(max_tokens=5, output_sequence_length=3)\n",
        "text_vec.adapt(dummy_vocab)\n",
        "print(text_vec(\"movie\"))\n",
        "#Q: why the zeros ? why shape 3?\n",
        "#Q: output type?"
      ],
      "metadata": {
        "id": "KjBz5gZ_qxrW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a41d7da-db6f-4ee9-a604-3805de386f14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([1 0 0], shape=(3,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Then obtain embeddings from text indices using the Embedding layer\n",
        "int_text = text_vec([\"movie was good\"])\n",
        "print(int_text)\n",
        "embedding = layers.Embedding(input_dim=5, output_dim=4)(int_text)  # Why 5 ?\n",
        "print(embedding)"
      ],
      "metadata": {
        "id": "FBVu6T8KSFVg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54ee3e90-077b-406d-b9bb-c0303592dff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[1 2 1]], shape=(1, 3), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[[ 0.01873472  0.04067269  0.03172047 -0.03395281]\n",
            "  [-0.04117931  0.04232437  0.04808872  0.01169556]\n",
            "  [ 0.01873472  0.04067269  0.03172047 -0.03395281]]], shape=(1, 3, 4), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Output from the attention function"
      ],
      "metadata": {
        "id": "h73JajZDY0xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute output of attention module\n",
        "attention_outputs = self_attention(embedding[0].numpy())\n",
        "print(attention_outputs.shape)\n",
        "print(attention_outputs)\n",
        "# These are z1, z2, z3 in the picture below"
      ],
      "metadata": {
        "id": "9Orj9lkLqxuk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9d9b620-30a7-4115-9610-a01c6b0dcaa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 4)\n",
            "[[-0.00122274  0.04122287  0.03717276 -0.01874726]\n",
            " [-0.00126232  0.04122396  0.03718357 -0.01871711]\n",
            " [-0.00122274  0.04122287  0.03717276 -0.01874726]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Eqn. with Queries, Keys and Values\n",
        "\n",
        "We computed the Self Attention based on the inputs of vectors themselves. This means that for fixed inputs, these attention weights would always be fixed. In other words, there are no learnable parameters. Need to introduce some learnable parmeters which will make the self attention mechanism more flexible and tunable for various tasks. To fullfil this purpose, three weight matices are introduced and multiplied with input $x_i$ seperately and three new terms **Queries(Q), Keys(K) and Values(V)** comes into picture as given by equations below. Vectorized implemenation  & Shape tracking are also shown along with equations."
      ],
      "metadata": {
        "id": "xEFymWCLeAQ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vectorized implemenation  & Shape tracking**\n",
        "\n",
        "$ d_{model} $ = Embedding vector for each word ( 512 as per the paper).\n",
        "\n",
        "$ X   \\Rightarrow (T \\times d_{model}) $\n",
        "\n",
        "\n",
        "$ Q = X W^{Q}   \\Rightarrow (T \\times d_{model}) \\times (d_{model} \\times d_k  )  \\Rightarrow   (T \\times d_{k}) $\n",
        "\n",
        "\n",
        "$ K = X W^{K}   \\Rightarrow (T \\times d_{model}) \\times (d_{model} \\times d_k  )  \\Rightarrow   (T \\times d_{k}) $\n",
        "\n",
        "\n",
        "$ V = X W^{V}   \\Rightarrow (T \\times d_{model}) \\times (d_{model} \\times d_v  )  \\Rightarrow   (T \\times d_{v}) $\n",
        "\n",
        "Dot product of Queries and Keys:\n",
        "\n",
        "$ Q K^{T}   \\Rightarrow (T \\times d_{k}) \\times (d_{k} \\times T  )  \\Rightarrow   (T \\times T) $\n",
        "\n",
        "T query vectors and T key vectors (Input Sequence), so need TxT attention weights. Make Sense! Taking SoftMax doesn't change the shape.\n",
        "\n",
        " **Shapes as per the paper**\n",
        "\n",
        "$\n",
        "\\begin{array}{|c|c|} \\hline\n",
        "Object   &  Shape & values  \\\\ \\hline\n",
        "q_i, k_i  &  d_k  &  (64,) \\\\\n",
        "v_i   &   d_v   &   (64,)  \\\\\n",
        "x_i   &   d_{model}   & (512,)  \\\\\n",
        "W^{Q}, W^{K}  &   d_{model} \\times d_k   &   (512, 64)  \\\\\n",
        "W^{V}   &   d_{model} \\times d_v   &  (512,64)  \\\\ \\hline\n",
        "\\end{array}\n",
        "$\n",
        "\n",
        "**Batch consideration**\n",
        "\n",
        "In code, a batch of N samples are processed at a time. Everyting would be  **N times**, like: $ N \\times T \\times d_k $ instead of just $ T \\times d_k$."
      ],
      "metadata": {
        "id": "nEqmUxkDf-fs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Fianl Scaled Dot Product Attention** equation inside each attention head with **Queries(Q)**, **Keys(Q)**, and **Values(V)**, which returns a Attention vector.\n",
        "\n",
        "<center>\n",
        "<img src= \"https://cdn.extras.talentsprint.com/aiml/Experiment_related_data/Images/Scaled_dot_product_Attention.png\" width=250px/>\n",
        "\n",
        "</center>\n",
        "\n",
        "\n",
        "$$Attention(Q, K, V) = softmax(\\frac{QK^T)}{\\sqrt{d_k}})V$$"
      ],
      "metadata": {
        "id": "OHr_TAfbgdig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multihead Attention"
      ],
      "metadata": {
        "id": "6PVKxy5PcCYE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the output of the final Encoder in the stack is passed to the Value and Key parameters in the Encoder-Decoder Attention.\n",
        "\n",
        "The Encoder-Decoder Attention is therefore getting a representation of both the target sequence (from the Decoder Self-Attention) and a representation of the input sequence (from the Encoder stack). It, therefore, produces a representation with the attention scores for each target sequence word that captures the influence of the attention scores from the input sequence as well.\n",
        "\n",
        "As this passes through all the Decoders in the stack, each Self-Attention and each Encoder-Decoder Attention also add their own attention scores into each words representation."
      ],
      "metadata": {
        "id": "yLiXoAWEaQS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the Transformer, the Attention module repeats its computations multiple times in parallel. Each of these is called an Attention Head. The Attention module splits its Query, Key, and Value parameters N-ways and passes each split independently through a separate Head. All of these similar Attention calculations are then combined together to produce a final Attention score. This is called Multi-head attention and gives the Transformer greater power to encode multiple relationships and nuances for each word."
      ],
      "metadata": {
        "id": "xzQy3FMLaj8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the paper, the diagram for a scaled dot product attention does not use any weights at all. Instead, the weights are included only in the multi head attention block, shown in figure below :\n",
        "<br><br>\n",
        "<center>\n",
        "<img src= \"https://cdn.extras.talentsprint.com/aiml/Experiment_related_data/Images/Multi_head_attention_with_weights.png\" width=900px/>\n",
        "</center>"
      ],
      "metadata": {
        "id": "xgX4dIqzalYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding the shape\n",
        "<br>\n",
        "<center>\n",
        "<img src= \"https://cdn.extras.talentsprint.com/aiml/Experiment_related_data/Images/Multi_head_attention_shape_tracking.png\" width=750px>\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "**Final Projection :** $ Output = concat(A_1, A_2, ..., A_h)  W^{o} $\n",
        "\n",
        "**Shape of :**  $ concat (A_1, A_2, ..., A_h) \\Rightarrow  (T \\times hd_v) $\n",
        "\n",
        "**Shape of:**  $  W^{o} \\Rightarrow (hd_v \\times d_{model}) $\n",
        "\n",
        "**Shape of final:**  $ Ouput = concat (A_1, A_2, ..., A_h) W^{o} \\Rightarrow  (T \\times hd_v) \\times (hd_v \\times d_{model})  \\Rightarrow  (T \\times d_{model}) \\Leftarrow $ **Back to the initial input shape.**\n",
        "\n",
        "Batch size is not displayed here."
      ],
      "metadata": {
        "id": "QydYglmilY9R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Encoder Block\n",
        "\n",
        "The Transformer Encoder consists of a stack of\n",
        " identical layers (Encoder Block) as shown in figure below, where each layer further consists of two main sub-layers:\n",
        "\n",
        "* The first sub-layer comprises a multi-head attention mechanism that receives the queries, keys, and values as inputs.\n",
        "* A second sub-layer comprises a fully-connected feed-forward network.\n",
        "\n",
        "Following each of these two sub-layers is layer normalization, into which the sub-layer input (through a residual/skip connection) and output are fed.\n",
        "\n",
        "Regularization is also intrpduced into the model by applying a dropout to the output of each sub-layer (before the layer normalization step), as well as to the positional encodings before these are fed into the encoder.\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "<img src=\"https://cdn.extras.talentsprint.com/aiml/Experiment_related_data//Images/Encoder_tfr_block_unfolded.png\"  width=600 px />$$\n",
        "<img src=\"https://cdn.extras.talentsprint.com/aiml/Experiment_related_data/Images/Encoder_tfr_block.png\" width=180 px/>\n",
        "\n",
        "</center>"
      ],
      "metadata": {
        "id": "9ei6127oCBQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The transformer encoder architecture typically consists of multiple layers, each of which includes a self-attention mechanism and a feed-forward neural network. The self-attention mechanism allows the model to weigh the importance of different input sequence parts by calculating the embeddings' dot product. This mechanism is also known as multi-head attention.\n",
        "\n",
        "The feed-forward network allows the model to extract higher-level features from the input. This network usually comprises two linear layers with a ReLU activation function in between. The feed-forward network allows the model to extract deeper meaning from the input data and more compactly and usefully represent the input.In the paper, an ANN with one hidden layer and a ReLu activation in the middle  with no activation function at output layer has been implemented.\n",
        "\n",
        "The transformer encoder is a crucial part of the transformer encoder-decoder architecture, which is widely used for natural language processing tasks."
      ],
      "metadata": {
        "id": "UU0hqgB0cz17"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder Transformer\n",
        "Stacking transormer blocks gives a Transfomer! Shown in figure below:\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "<img src= \"https://cdn.extras.talentsprint.com/aiml/Experiment_related_data/Images/Encoder_transfomer.png\" width=1000px/>\n",
        "</center>"
      ],
      "metadata": {
        "id": "RWF4OyFonkHe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define TransformerEncoder class"
      ],
      "metadata": {
        "id": "MxZdesd1dgAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim    # Dimension of embedding. 4 in the dummy example\n",
        "        self.dense_dim = dense_dim    # No. of neurons in dense layer\n",
        "        self.num_heads = num_heads    # No. of heads for MultiHead Attention layer\n",
        "        self.attention = layers.MultiHeadAttention(# MultiHead Attention layer -\n",
        "            num_heads=num_heads, key_dim=embed_dim)   # see coloured pic above\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]    # encoders are stacked on top of the other.\n",
        "        )                                 # So output dimension is also embed_dim\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    # Call function based on figure above\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]   # Will discuss in next tutorial\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)  # Query: inputs, Value: inputs, Keys: Same as Values by default\n",
        "                                                  # Q: Can you see how this is self attention?\n",
        "        proj_input = self.layernorm_1(inputs + attention_output) # LayerNormalization; + Recall cat picture\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)  # LayerNormalization + Residual connection\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "loHOqli9qxza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model definition"
      ],
      "metadata": {
        "id": "QX4opYwUdrtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the Transformer encoder\n",
        "vocab_size = 20000\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "61zmVZv8q4AO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aaaeeba-2695-45bc-f9b5-cfcc0c41b5b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     (None, None, 256)         5120000   \n",
            "                                                                 \n",
            " transformer_encoder (Trans  (None, None, 256)         543776    \n",
            " formerEncoder)                                                  \n",
            "                                                                 \n",
            " global_max_pooling1d (Glob  (None, 256)               0         \n",
            " alMaxPooling1D)                                                 \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5664033 (21.61 MB)\n",
            "Trainable params: 5664033 (21.61 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train and evaluate the performance of the model"
      ],
      "metadata": {
        "id": "RwJwGlMPdypN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"transformer_encoder.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
        "model = keras.models.load_model(\n",
        "    \"transformer_encoder.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder})\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "id": "GEs7SvpGq6VU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bb9f30f-1ebe-40e5-aa2b-72d950afab85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "625/625 [==============================] - 58s 89ms/step - loss: 0.4962 - accuracy: 0.7692 - val_loss: 0.3534 - val_accuracy: 0.8424\n",
            "Epoch 2/20\n",
            "625/625 [==============================] - 48s 77ms/step - loss: 0.3407 - accuracy: 0.8541 - val_loss: 0.3270 - val_accuracy: 0.8548\n",
            "Epoch 3/20\n",
            "625/625 [==============================] - 45s 72ms/step - loss: 0.2978 - accuracy: 0.8727 - val_loss: 0.3062 - val_accuracy: 0.8664\n",
            "Epoch 4/20\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 0.2701 - accuracy: 0.8895 - val_loss: 0.3066 - val_accuracy: 0.8746\n",
            "Epoch 5/20\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 0.2410 - accuracy: 0.9013 - val_loss: 0.2983 - val_accuracy: 0.8746\n",
            "Epoch 6/20\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 0.2106 - accuracy: 0.9166 - val_loss: 0.3084 - val_accuracy: 0.8734\n",
            "Epoch 7/20\n",
            "625/625 [==============================] - 43s 68ms/step - loss: 0.1797 - accuracy: 0.9312 - val_loss: 0.3125 - val_accuracy: 0.8784\n",
            "Epoch 8/20\n",
            "625/625 [==============================] - 42s 68ms/step - loss: 0.1503 - accuracy: 0.9441 - val_loss: 0.3846 - val_accuracy: 0.8620\n",
            "Epoch 9/20\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 0.1251 - accuracy: 0.9543 - val_loss: 0.3570 - val_accuracy: 0.8716\n",
            "Epoch 10/20\n",
            "625/625 [==============================] - 42s 67ms/step - loss: 0.1033 - accuracy: 0.9628 - val_loss: 0.3946 - val_accuracy: 0.8722\n",
            "Epoch 11/20\n",
            "625/625 [==============================] - 42s 68ms/step - loss: 0.0841 - accuracy: 0.9703 - val_loss: 0.3823 - val_accuracy: 0.8716\n",
            "Epoch 12/20\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.0624 - accuracy: 0.9782 - val_loss: 0.4437 - val_accuracy: 0.8658\n",
            "Epoch 13/20\n",
            "625/625 [==============================] - 42s 67ms/step - loss: 0.0486 - accuracy: 0.9833 - val_loss: 0.4690 - val_accuracy: 0.8746\n",
            "Epoch 14/20\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.0422 - accuracy: 0.9845 - val_loss: 0.5355 - val_accuracy: 0.8638\n",
            "Epoch 15/20\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.0353 - accuracy: 0.9869 - val_loss: 0.5424 - val_accuracy: 0.8712\n",
            "Epoch 16/20\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.0267 - accuracy: 0.9905 - val_loss: 0.5910 - val_accuracy: 0.8652\n",
            "Epoch 17/20\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.0231 - accuracy: 0.9926 - val_loss: 0.6261 - val_accuracy: 0.8592\n",
            "Epoch 18/20\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.0208 - accuracy: 0.9924 - val_loss: 0.6749 - val_accuracy: 0.8668\n",
            "Epoch 19/20\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.0197 - accuracy: 0.9934 - val_loss: 0.7519 - val_accuracy: 0.8564\n",
            "Epoch 20/20\n",
            "625/625 [==============================] - 43s 70ms/step - loss: 0.0170 - accuracy: 0.9947 - val_loss: 0.6830 - val_accuracy: 0.8668\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.3010 - accuracy: 0.8735\n",
            "Test acc: 0.873\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Embedding\n",
        "\n",
        "**Positional Embedding = Word Embedding + Positional Encoding**\n",
        "\n",
        "**Positional Encoding**\n",
        "\n",
        "Passing embeddings directly into the transformer block results in missing of information about the order of tokens. As attention is permutation invariant i.e. order of token does not matter to attention.\n",
        "Although transformers are a sequence model, it appears that this important detail has somehow been lost. Positional encoding is for rescue.\n",
        "\n",
        "Positional encoding add positional information to the existing embeddings.\n",
        "\n",
        "**A unique set of numbers added at each position of the existing embeddings**, such that this new set of numbers can uniquely identify which postion they are located at.\n",
        "\n",
        "\n",
        "1. Positional Encoding by SubClassing the Keras Embedding Layer (Trainable)\n",
        "2. Positional Encoding scheme as per the paper (Non-Trainable)\n",
        "\n",
        " In this scheme the encoding is created by using a set of sins and cosines at different frequencies. The  paper uses the following formula for calculating the positional encoding. [Positional Encoding Vizualization.](https://erdem.pl/2021/05/understanding-positional-encoding-in-transformers)\n",
        "\n",
        "$$\\Large{PE_{(pos, 2i)} = \\sin(pos / 10000^{2i / d_{model}})} $$\n",
        "$$\\Large{PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i / d_{model}})} $$\n",
        "\n",
        "We are going to implement Positional Encoding by SubClassing the Keras Embedding Layer (Trainable). Thus Instead of using the Embedding layer from keras define a PositionalEmbedding class and create a new model using it as the embedding layer."
      ],
      "metadata": {
        "id": "jFZ23Yzywtqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using positional encoding to re-inject order information\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "B1MVsKFZsVPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Definition"
      ],
      "metadata": {
        "id": "ym7HzyaLeSrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Combining the Transformer encoder with positional embedding\n",
        "\n",
        "vocab_size = 20000\n",
        "sequence_length = 600\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "BOFIwLWGsk_A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5ee1d12-cf9a-405d-9f8a-e7ef4201ee9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " positional_embedding (Posi  (None, None, 256)         5273600   \n",
            " tionalEmbedding)                                                \n",
            "                                                                 \n",
            " transformer_encoder_1 (Tra  (None, None, 256)         543776    \n",
            " nsformerEncoder)                                                \n",
            "                                                                 \n",
            " global_max_pooling1d_1 (Gl  (None, 256)               0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5817633 (22.19 MB)\n",
            "Trainable params: 5817633 (22.19 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train and evaluate the model"
      ],
      "metadata": {
        "id": "1_Ua8m85eWw9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dro7yrtYtP5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be81d314-98e1-4a8f-c767-ceda3713fd84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "625/625 [==============================] - 62s 95ms/step - loss: 0.5329 - accuracy: 0.7403 - val_loss: 0.3065 - val_accuracy: 0.8676\n",
            "Epoch 2/20\n",
            "625/625 [==============================] - 50s 81ms/step - loss: 0.3054 - accuracy: 0.8709 - val_loss: 0.6501 - val_accuracy: 0.7950\n",
            "Epoch 3/20\n",
            "625/625 [==============================] - 47s 76ms/step - loss: 0.2385 - accuracy: 0.9053 - val_loss: 0.2848 - val_accuracy: 0.8874\n",
            "Epoch 4/20\n",
            "625/625 [==============================] - 47s 76ms/step - loss: 0.1954 - accuracy: 0.9234 - val_loss: 0.3392 - val_accuracy: 0.8852\n",
            "Epoch 5/20\n",
            "625/625 [==============================] - 45s 72ms/step - loss: 0.1648 - accuracy: 0.9382 - val_loss: 0.3832 - val_accuracy: 0.8878\n",
            "Epoch 6/20\n",
            "625/625 [==============================] - 45s 72ms/step - loss: 0.1355 - accuracy: 0.9484 - val_loss: 0.8359 - val_accuracy: 0.8290\n",
            "Epoch 7/20\n",
            "625/625 [==============================] - 46s 73ms/step - loss: 0.1153 - accuracy: 0.9582 - val_loss: 0.5718 - val_accuracy: 0.8758\n",
            "Epoch 8/20\n",
            "625/625 [==============================] - 46s 74ms/step - loss: 0.0918 - accuracy: 0.9689 - val_loss: 0.6156 - val_accuracy: 0.8764\n",
            "Epoch 9/20\n",
            "625/625 [==============================] - 45s 73ms/step - loss: 0.0741 - accuracy: 0.9739 - val_loss: 0.6219 - val_accuracy: 0.8716\n",
            "Epoch 10/20\n",
            "625/625 [==============================] - 45s 72ms/step - loss: 0.0573 - accuracy: 0.9804 - val_loss: 0.6223 - val_accuracy: 0.8738\n",
            "Epoch 11/20\n",
            "625/625 [==============================] - 45s 72ms/step - loss: 0.0468 - accuracy: 0.9841 - val_loss: 0.6438 - val_accuracy: 0.8726\n",
            "Epoch 12/20\n",
            "625/625 [==============================] - 45s 71ms/step - loss: 0.0397 - accuracy: 0.9869 - val_loss: 0.8116 - val_accuracy: 0.8736\n",
            "Epoch 13/20\n",
            "625/625 [==============================] - 46s 73ms/step - loss: 0.0265 - accuracy: 0.9918 - val_loss: 0.7423 - val_accuracy: 0.8700\n",
            "Epoch 14/20\n",
            "625/625 [==============================] - 46s 73ms/step - loss: 0.0261 - accuracy: 0.9916 - val_loss: 0.8683 - val_accuracy: 0.8708\n",
            "Epoch 15/20\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.0219 - accuracy: 0.9934 - val_loss: 0.9595 - val_accuracy: 0.8700\n",
            "Epoch 16/20\n",
            "625/625 [==============================] - 45s 73ms/step - loss: 0.0170 - accuracy: 0.9944 - val_loss: 1.1469 - val_accuracy: 0.8684\n",
            "Epoch 17/20\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.0131 - accuracy: 0.9962 - val_loss: 1.0430 - val_accuracy: 0.8718\n",
            "Epoch 18/20\n",
            "625/625 [==============================] - 45s 73ms/step - loss: 0.0148 - accuracy: 0.9952 - val_loss: 1.1788 - val_accuracy: 0.8668\n",
            "Epoch 19/20\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 0.0120 - accuracy: 0.9969 - val_loss: 1.1704 - val_accuracy: 0.8718\n",
            "Epoch 20/20\n",
            "625/625 [==============================] - 45s 71ms/step - loss: 0.0110 - accuracy: 0.9962 - val_loss: 1.1911 - val_accuracy: 0.8704\n",
            "782/782 [==============================] - 23s 29ms/step - loss: 0.2935 - accuracy: 0.8805\n",
            "Test acc: 0.880\n"
          ]
        }
      ],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
        "model = keras.models.load_model(\n",
        "    \"full_transformer_encoder.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
        "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "If you are very interested or plan to work closely on Transformers, then following are good resource that explains in simplified manner.\n",
        "\n",
        "1. [Attention Is All You Need](https://arxiv.org/pdf/1706.03762v6.pdf)\n",
        "\n",
        "2. [Understanding Positional  Encoding](https://erdem.pl/2021/05/understanding-positional-encoding-in-transformers)\n",
        "\n",
        "2. [Implement Multi-Head Attention](https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras)\n",
        "\n",
        "3. [Implementing the Transformer Encoder](https://machinelearningmastery.com/implementing-the-transformer-encoder-from-scratch-in-tensorflow-and-keras/)\n",
        "\n",
        "4. [Illustrated-transformer](https://jalammar.github.io/illustrated-transformer/)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "beX9CfsVI3on"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOC1n4C5dxsb"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HQodIl8dxs1"
      },
      "outputs": [],
      "source": [
        "#@title Which layer in the Transformer model is responsible for capturing local dependencies within an input sequence? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"\" #@param [\"\", \"Self-Attention Layer\", \"Feedforward Neural Network Layer\", \"Positional Encoding Layer\", \"Layer Normalization\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "outputs": [],
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "outputs": [],
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "outputs": [],
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "outputs": [],
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "outputs": [],
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ]
    }
  ]
}