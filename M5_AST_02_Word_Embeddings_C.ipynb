{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpatra85/ColabTF_EDU/blob/master/M5_AST_02_Word_Embeddings_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNgLag1Euy3H"
      },
      "source": [
        "# Advanced Certification Programme in AI and MLOps\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment 2: Word2vec, GloVe Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tdtrlAhvIHY"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "* understand and perform text pre-processing\n",
        "* train a Word2Vec model and save it in a file\n",
        "* load the saved model to get the vector representation of words\n",
        "* measure and plot the similarity between the words\n",
        "* use the pre-trained GloVe Embeddings to plot the similarity between the words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Embedding\n",
        "\n",
        "Here we will learn to deal with textual data, we need to convert it into numbers before feeding it into any machine learning model. For simplicity, words can be compared to categorical variables. We use one-hot encoding to convert categorical features into numbers. To do so, we create dummy features for each of the category and populate them with 0's and 1's.\n",
        "\n",
        "Similarly, if we use one-hot encoding on words in textual data, we will have a dummy feature for each word, which means 10,000 features for a vocabulary of 10,000 words. This is not a feasible embedding approach as it demands large storage space for the word vectors and reduces model efficiency and no relation is captured between words.\n",
        "\n",
        "Some of the most popular techniques to learn word embeddings includes:\n",
        "- Word2Vec\n",
        "- GloVe"
      ],
      "metadata": {
        "id": "okovJktL_Ea7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25xeb09mMs0B"
      },
      "source": [
        "## Dataset Description\n",
        "\n",
        "The IMDB movie review dataset can be downloaded from [here](http://ai.stanford.edu/~amaas/data/sentiment/). This dataset for binary sentiment classification contains around 50k movie reviews with the following attributes:\n",
        "\n",
        "* **review:** text based review of each movie\n",
        "* **sentiment:** positive or negative sentiment value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZLYIcdJffR4"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vY9tgfHgffSY"
      },
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Hlj81lsuffSZ"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M5_AST_02_Word_Embeddings_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/IMDB_Dataset.csv\")\n",
        "    ipython.magic(\"sx wget https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/glove.6B.zip\")\n",
        "    ipython.magic(\"sx unzip glove.6B.zip\")\n",
        "    ipython.magic(\"sx wget https://cdn.talentsprint.com/talentsprint1/archives/sc/aiml/experiment_related_data/AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD.rar\")\n",
        "    ipython.magic(\"sx unrar e /content/AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD.rar\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://aimlops-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE THAT ABOVE CELL MIGHT TAKE SOME TIME TO RUN AS IT IS DOWNLOADING THE NECESSARY DATA FILES!**"
      ],
      "metadata": {
        "id": "uqRrg1HunuM-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RH8Ecq9sbYU"
      },
      "source": [
        "### Importing required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFl76_ngsasw"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords     # to get collection of stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "import gensim    # Word to Vec\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer           # to encode text to int\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences      # to do padding or truncating\n",
        "from keras.models import Sequential                   # the model\n",
        "import pprint       # pprint is a native Python library that allows to customize the formatting of output\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdT0aESgsVPU"
      },
      "source": [
        "### Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAzdHqulsRbv"
      },
      "source": [
        "movie_reviews = pd.read_csv(\"IMDB_Dataset.csv\")\n",
        "\n",
        "# Check for null values\n",
        "movie_reviews.isnull().values.any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r79To9Q4OGiI"
      },
      "source": [
        "print(movie_reviews.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJcLZO2aN_kl"
      },
      "source": [
        "# Print the first five rows from the data\n",
        "movie_reviews.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unique values for sentiment\n",
        "movie_reviews.sentiment.unique()"
      ],
      "metadata": {
        "id": "ny6ybXPM9wRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count for each sentiment\n",
        "movie_reviews.sentiment.value_counts()"
      ],
      "metadata": {
        "id": "_ux1-rcC9Mqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the postive and negative sentiments\n",
        "movie_reviews.sentiment.value_counts().plot.bar(ylim=0);"
      ],
      "metadata": {
        "id": "NuTf5Ek_9rAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ol2PIDSMtCwN"
      },
      "source": [
        "# Let us view one of the reviews\n",
        "movie_reviews[\"review\"][5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMIU5YtCv0k8"
      },
      "source": [
        "### Data pre-processing\n",
        "\n",
        "For the text data in review column, we will perform below pre-processing steps:\n",
        "- removing html tags\n",
        "- removing non alphabets (punctuations and numbers)\n",
        "- removing stop words\n",
        "- ignore words whose length is less than 2\n",
        "- convert the text to lower-case\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8J-xAfCEQej"
      },
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "def preprocess_text(sen):\n",
        "\n",
        "    sen = re.sub('<.*?>', ' ', sen)                        # remove html tags\n",
        "    tokens = word_tokenize(sen)                            # tokenize words\n",
        "    tokens = [w.lower() for w in tokens]                   # convert to lower case\n",
        "    table = str.maketrans('', '', string.punctuation)      # remove punctuations\n",
        "    stripped = [w.translate(table) for w in tokens]\n",
        "\n",
        "    words = [word for word in stripped if word.isalpha()]  # remove non-alphabet\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    words = [w for w in words if not w in stop_words]      # remove stop words\n",
        "    words = [w for w in words if len(w) > 2]               # Ignore words whose length is less than 2\n",
        "\n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjzvZSmwvO5O"
      },
      "source": [
        "# Store the preprocessed reviews in a new list\n",
        "review_lines = movie_reviews['review'].apply(preprocess_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqkMzMjrAe4V"
      },
      "source": [
        "# Check for the length of the preprocessed text\n",
        "len(review_lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viX5aijFhlGf"
      },
      "source": [
        "# Print the preprocessed text for the first review\n",
        "print(review_lines[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(review_lines[1])"
      ],
      "metadata": {
        "id": "ZiLI10b8gN_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-kFMtmcv6as"
      },
      "source": [
        "# Now let’s convert the sentiment from string to a binary form of 1 and 0,\n",
        "# where 1 is for ‘positive’ sentiment and 0 for ‘negative’.\n",
        "y = movie_reviews['sentiment'].apply(lambda x: 1 if x==\"positive\" else 0)\n",
        "\n",
        "y[0:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec\n",
        "\n",
        "It is one of the most popular techniques to learn word embeddings. It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc. A word embedding is a learned representation for text where words that have the same meaning have a similar representation."
      ],
      "metadata": {
        "id": "nCm0bhPFrISV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Why do we need them?\n",
        "\n",
        "Consider the following similar sentences: **Have a good day** and **Have a great day**. They hardly have different meaning. If we construct an vocabulary (let’s call it V), it would have V = **{Have, a, good, great, day}**.\n",
        "\n",
        "Now, let us create a one-hot encoded vector for each of these words in V. Length of our one-hot encoded vector would be equal to the size of V (=5). We would have a vector of zeros except for the element at the index representing the corresponding word in the vocabulary. That particular element would be one. The encodings below would explain this better.\n",
        "\n",
        "Have = [1,0,0,0,0] ; a = [0,1,0,0,0] ; good = [0,0,1,0,0] ; great = [0,0,0,1,0] ; day = [0,0,0,0,1]\n",
        "\n",
        "If we try to visualize these encodings, we can think of a 5 dimensional space, where each word occupies one of the dimensions and has nothing to do with the rest (no projection along the other dimensions). This means ‘good’ and ‘great’ are as different as ‘day’ and ‘have’, which is not true.\n",
        "\n",
        "Our objective is to have words with similar context occupy close spatial positions. Mathematically, the **cosine** of the angle between such vectors should be close to 1, i.e. angle close to 0. Higher the cosine similarity, the words are more closer"
      ],
      "metadata": {
        "id": "KQFzcN1KrNQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cosine Similarity**\n",
        "\n",
        "$sim(A, B) = cos(\\theta) = \\frac{\\bar{A}. \\bar{B}}{\\bar{|A|}\\bar{|B|}}$\n",
        "\n",
        "\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/Word_Embedding.png\" width=\"650\" height=\"450\">\n",
        "</center>"
      ],
      "metadata": {
        "id": "0tFYZn2srhHA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ7P01yMVcai"
      },
      "source": [
        "**Word2vec** model has 2 algorithms:\n",
        "\n",
        "1. **Continuous bag of word (CBOW):**\n",
        "\n",
        "    CBOW predicts the target words from the surrounding context words. **Eg: Context word:** \"The cat sits on the ..\",  **Target word:** \"mat\"\n",
        "\n",
        "2. **Skip-gram:**\n",
        "\n",
        "    Skip-gram predicts surrounding context words from the target words. **Eg: Context word:** \"The cat ... on the mat\",  **Target word:** \"sat\"\n",
        "\n",
        "**Note:** For more details of word2vec model refer to the following [link](https://medium.com/@zafaralibagh6/a-simple-word2vec-tutorial-61e64e38a6a1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fn42IGncfqFI"
      },
      "source": [
        "### Train word2vec model to obtain word embeddings\n",
        "\n",
        "We will use Gensim to  implement the Word2Vec. **Gensim** is an open source Python library for natural language processing. It is developed and is maintained by the Czech natural language processing researcher Radim Řehůřek and his company RaRe Technologies. Here, the first step is to prepare the text corpus for learning the embedding by creating word tokens, removing punctuation, removing stop words etc. The word2vec algorithm processes documents sentence by sentence.\n",
        "\n",
        "The dataset is already preprocessed. The `review_lines` contains the text corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY_cX3GeA3Z0"
      },
      "source": [
        "EMBEDDING_DIM = 100\n",
        "# Train word2vec model after preprocessing the reviews\n",
        "model = gensim.models.Word2Vec(sentences = review_lines,\n",
        "                               max_vocab_size=100000,\n",
        "                               window=1,\n",
        "                               vector_size=EMBEDDING_DIM,\n",
        "                               workers=4,\n",
        "                               min_count=1,\n",
        "                               sg = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEovOd7rglXj"
      },
      "source": [
        "Parameters for Word2Vec:\n",
        "\n",
        "- **sentences:** List of sentences; here we pass the list of review sentences.\n",
        "\n",
        "- **vector_size:** The number of dimensions in which we want to represent our word. This is the size of the word vector which instructs the Word2Vec() method to create a vector size of 100\n",
        "\n",
        "- **min_count:** Word with frequency greater than min_count only are going to be included into the model. Usually, the bigger and more extensive your text, the higher this number can be.\n",
        "\n",
        "- **window:** Only terms that occur within a window-neighborhood of a term, in a sentence, are associated with it during training. The usual value is 4 or 5.\n",
        "\n",
        "- **workers:** Number of threads used in training parallelization, to speed up training.\n",
        "\n",
        "- **sg:** {0, 1} Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
        "\n",
        "\n",
        "To know more about the the parameters of gensim.models.Word2Vec, click [here](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYQgWDH3hFp9"
      },
      "source": [
        "### Test Word2Vec Model\n",
        "\n",
        "Try some word embeddings the model learnt from the movie review dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZ5ZbiLMhP6Q"
      },
      "source": [
        "The most similar words for word 'good' are:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Weui1-5lEBRB"
      },
      "source": [
        "model.wv.most_similar('good')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU-Iqg5vh7nQ"
      },
      "source": [
        "The process of creating word embeddings by training a Word2Vec model has been discussed so far. This model can be saved to be used later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdUrraXRJOy5"
      },
      "source": [
        "# Save model\n",
        "filename = \"imdb_embedding_word2vec.txt\"\n",
        "model.wv.save_word2vec_format(filename, binary=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next part, pre-trained word embeddings will be used to get an intuitive plot."
      ],
      "metadata": {
        "id": "1Hc4-nMS-G63"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymsi9IR9OEKC"
      },
      "source": [
        "### Use Pre-trained Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Google pre-trained word2vec model**\n",
        "\n",
        "Google has published a pre-trained word2vec model. It is trained on part of Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. For more information about the word2vec model published by Google, you can see the link [here](https://code.google.com/archive/p/word2vec/)."
      ],
      "metadata": {
        "id": "UH12VidyWw0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the pre-trained word embedding saved in file `AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD.bin`."
      ],
      "metadata": {
        "id": "3wE9TxZC-twI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Google news 300 vectors file\n",
        "model_plot = gensim.models.KeyedVectors.load_word2vec_format('AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD.bin', binary=True, limit=500000)"
      ],
      "metadata": {
        "id": "p18GwFnMAHMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of these words is specifically chosen to get the intuitive plot\n",
        "words = ['king', 'queen', 'river', 'water', 'ocean', 'tree', 'leaf', 'happy', 'glad', 'mother', 'daughter']"
      ],
      "metadata": {
        "id": "hVHVWAIbX8UE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a PrettyPrinter() object\n",
        "pp = pprint.PrettyPrinter()\n",
        "\n",
        "# Vector representation of a specific word\n",
        "print(\"Size of the vector is\", len(model_plot[\"king\"]))\n",
        "pp.pprint(model_plot[\"king\"])"
      ],
      "metadata": {
        "id": "Z-jIXR83GLC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vector representation of each word using Word2Vec\n",
        "word2vec = []\n",
        "\n",
        "for word in words:\n",
        "    try:\n",
        "        word2vec.append(model_plot[word])\n",
        "    except:\n",
        "        pass\n",
        "print(\"There are %d words and the vector size of each word is %d\" %(len(word2vec),len(word2vec[0])))"
      ],
      "metadata": {
        "id": "d13wqYtZJUGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w23olxol8BIf"
      },
      "source": [
        "### Measure the similarity between the words using cosine_similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_similarity = []\n",
        "\n",
        "for i, word_1 in enumerate(words):\n",
        "    w2v_row_wise_simiarity = []\n",
        "    for j, word_2 in enumerate(words):\n",
        "        # Get the vectors of the word using Word2Vec\n",
        "        vec_1, vec_2 = model_plot[word_1], model_plot[word_2]\n",
        "\n",
        "        # As the vectors are in one dimensional, convert it to 2D by reshaping\n",
        "        vec_1, vec_2 = np.array(vec_1).reshape(1,-1), np.array(vec_2).reshape(1,-1)\n",
        "\n",
        "        # Measure the cosine similarity between two vectors\n",
        "        similarity = cosine_similarity(vec_1,vec_2)\n",
        "        w2v_row_wise_simiarity.append(np.array(similarity).item())\n",
        "\n",
        "    # Store the cosine similarity values in a list\n",
        "    w2v_similarity.append(w2v_row_wise_simiarity)\n",
        "\n",
        "pd.DataFrame(w2v_similarity, columns = words, index = words)"
      ],
      "metadata": {
        "id": "8BmU3uD3KT58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize similarity using heatmap"
      ],
      "metadata": {
        "id": "-XwsqmZnBpSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(pd.DataFrame(w2v_similarity, columns = words, index = words))"
      ],
      "metadata": {
        "id": "uVrN6GJyKkB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Higher the cosine similarity, the words are more closer"
      ],
      "metadata": {
        "id": "2SmIn1PFCzUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize the words in 2D-plane by reducing the dimensions using PCA"
      ],
      "metadata": {
        "id": "LC2LpnlACCzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a 2-dimensional PCA model of the word vectors using the scikit-learn PCA class\n",
        "# n_components in PCA specifies the no.of dimensions\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# Fit and transform the vectors using PCA model\n",
        "reduced_w2v = pca.fit_transform(word2vec)"
      ],
      "metadata": {
        "id": "60tkmOdLLF7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "plt.scatter(reduced_w2v[:,0],reduced_w2v[:,1], s = 12, color = 'red')\n",
        "plt.xlim([-2.5,2.5])\n",
        "plt.ylim([-2.5,2.5])\n",
        "x, y = reduced_w2v[:,0] , reduced_w2v[:,1]\n",
        "for i in range(len(x)):\n",
        "    plt.annotate(words[i],xy=(x[i], y[i]),xytext=(x[i]+0.05,y[i]+0.05))"
      ],
      "metadata": {
        "id": "zZnZUMZXYhY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above plot, it can be seen that tree leaf are more related, water river ocear are more related, and so on."
      ],
      "metadata": {
        "id": "2TIiDmRtCMOO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GloVe\n",
        "\n",
        "  GloVe stands for “Global Vectors” for word representation. It is developed by Stanford for generating word embeddings. GloVe captures both global statistics and local statistics of a corpus, in order to come up with word vectors.\n"
      ],
      "metadata": {
        "id": "NT0BRZL7RfPr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using the pre-trained GloVe model"
      ],
      "metadata": {
        "id": "IxW18y6vBRSZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZt0USXAZVnk"
      },
      "source": [
        "GloVe_Dict = {}\n",
        "# Loading the 50-dimensional vector of the model\n",
        "with open(\"glove.6B.50d.txt\", 'r') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], \"float32\")\n",
        "        GloVe_Dict[word] = vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEswIOf9xeAg"
      },
      "source": [
        "# Length of the word vocabulary\n",
        "print(len(GloVe_Dict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vector representation of a specific word\n",
        "print(\"Size of the vector is\", len(GloVe_Dict[\"king\"]))\n",
        "pp.pprint(GloVe_Dict[\"king\"])"
      ],
      "metadata": {
        "id": "mIONlKMZVcJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vector representation of each word using GloVe\n",
        "vectors = []\n",
        "for word in words:\n",
        "    try:\n",
        "        vector = GloVe_Dict[word]\n",
        "        vectors.append(vector)\n",
        "    except:\n",
        "        pass\n",
        "print(\"There are %d words and the vector size of each word is %d\" %((len(vectors),len(vectors[0]))))"
      ],
      "metadata": {
        "id": "iNqgAxf-Viu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J8Kjgo_PaKh"
      },
      "source": [
        "### Measure the similarity between the words using cosine_similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_similarity = []\n",
        "for i, word_1 in enumerate(words):\n",
        "    row_wise_simiarity = []\n",
        "    for j, word_2 in enumerate(words):\n",
        "        # Get the vectors of the word using GloVe\n",
        "        vec_1, vec_2 = GloVe_Dict[word_1], GloVe_Dict[word_2]\n",
        "\n",
        "        # As the vectors are in one dimensional, convert it to 2D by reshaping\n",
        "        vec_1, vec_2 = np.array(vec_1).reshape(1,-1), np.array(vec_2).reshape(1,-1)\n",
        "\n",
        "        # Measure the cosine similarity between the vectors.\n",
        "        similarity = cosine_similarity(vec_1, vec_2)\n",
        "        row_wise_simiarity.append(np.array(similarity).item())\n",
        "\n",
        "    # Store the cosine similarity values in a list\n",
        "    word_similarity.append(row_wise_simiarity)\n",
        "\n",
        "# Create a DataFrame to view the similarity between words\n",
        "pd.DataFrame(word_similarity, columns=words, index=words)"
      ],
      "metadata": {
        "id": "xMPsOKc6XPNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize similarity using heatmap"
      ],
      "metadata": {
        "id": "AiC6jQPaCQMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(pd.DataFrame(word_similarity, columns=words, index=words))"
      ],
      "metadata": {
        "id": "KEYx2ECwXTms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GloVe derives the semantic relationship between the words. Higher the cosine similarity, the words are relatively closer"
      ],
      "metadata": {
        "id": "asdrSoL-DOo8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize the words in 2D-plane by reducing the dimensions using PCA"
      ],
      "metadata": {
        "id": "OOGROPnACaGl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVPYkoF5S9rm"
      },
      "source": [
        "# Create a 2-dimensional PCA model of the word vectors using the scikit-learn PCA class\n",
        "# n_components in PCA specifies the no.of dimensions\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# Fit and transform the vectors using PCA model\n",
        "reduced_vectors = pca.fit_transform(vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QnczXr2BLNj"
      },
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "plt.scatter(reduced_vectors[:,0],reduced_vectors[:,1], s = 12, color = 'red')\n",
        "plt.xlim([-3.5,4.5])\n",
        "plt.ylim([-3.5,3.5])\n",
        "x, y = reduced_vectors[:,0] , reduced_vectors[:,1]\n",
        "for i in range(len(x)):\n",
        "    plt.annotate(words[i],xy=(x[i], y[i]),xytext=(x[i]+0.05,y[i]+0.05))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgSwVENIPcM6"
      },
      "source": [
        "#@title Which technique is used to address the issue of rare words in word2vec and GloVe embeddings? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"\" #@param [\"\", \"Subword tokenization\", \"Reducing the context window size\", \"Applying dimensionality reduction techniques\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ym68JPOvHPm"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr_w2auyvHPn"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRwFaM1DvHPo"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ihp2ejlNvHPo"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbJ6ik8mvHPp"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "N2DKhsDNvHPp"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}